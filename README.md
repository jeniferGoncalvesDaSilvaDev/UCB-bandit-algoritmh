# üé∞ Simula√ß√£o Interativa de Algoritmos Multi-Armed Bandit

Uma aplica√ß√£o web interativa desenvolvida em Streamlit para simular e comparar algoritmos de Multi-Armed Bandit, incluindo UCB1 e Epsilon-Greedy.

## üìã Sobre o Projeto

Este projeto implementa uma simula√ß√£o completa do problema cl√°ssico de Multi-Armed Bandit, permitindo que usu√°rios experimentem com diferentes algoritmos e par√¢metros para entender como eles se comportam em cen√°rios de explora√ß√£o vs. explora√ß√£o.

### Algoritmos Implementados

#### UCB1 (Upper Confidence Bound)
- **F√≥rmula**: Œº·µ¢ + ‚àö(2ln(t)/n·µ¢)
- **Estrat√©gia**: Equilibra explora√ß√£o e explora√ß√£o usando intervalos de confian√ßa
- **Garantias**: Inicializa com uma puxada por bra√ßo, depois usa o √≠ndice UCB
- **Vantagens**: Garantias te√≥ricas de regret logar√≠tmico

#### Epsilon-Greedy
- **F√≥rmula**: Com probabilidade Œµ explora aleatoriamente, sen√£o explora o melhor bra√ßo
- **Estrat√©gia**: Simples mas efetivo para muitos cen√°rios
- **Par√¢metro**: Œµ controla a taxa de explora√ß√£o (0.01 a 0.5)
- **Vantagens**: Simplicidade e facilidade de implementa√ß√£o

#### Thompson Sampling
- **F√≥rmula**: Amostragem de distribui√ß√µes Beta para cada bra√ßo
- **Estrat√©gia**: Abordagem Bayesiana que modela incerteza usando distribui√ß√µes de probabilidade
- **Par√¢metros**: Œ± (prior de sucessos) e Œ≤ (prior de falhas)
- **Vantagens**: Explora√ß√£o natural atrav√©s de incerteza probabil√≠stica

#### Gradient Bandit
- **F√≥rmula**: Softmax sobre prefer√™ncias aprendidas com gradiente estoc√°stico
- **Estrat√©gia**: Aprende prefer√™ncias (n√£o valores) e usa baseline de recompensa
- **Par√¢metro**: Learning rate (Œ±) para atualiza√ß√£o de prefer√™ncias
- **Vantagens**: Abordagem baseada em pol√≠tica, robusta a mudan√ßas de escala de recompensa

## üöÄ Funcionalidades

### Simula√ß√£o Individual
- Execute qualquer um dos 4 algoritmos separadamente:
  - UCB1
  - Epsilon-Greedy
  - Thompson Sampling
  - Gradient Bandit
- Visualize regret m√©dio e porcentagem de a√ß√µes √≥timas ao longo do tempo
- Analise estat√≠sticas detalhadas de cada bra√ßo

### Modo Compara√ß√£o Completa
- Execute todos os 4 algoritmos simultaneamente
- Compare performance lado a lado
- Mesmo ambiente para compara√ß√£o justa
- Gr√°ficos sobrepostos para an√°lise visual f√°cil

### Par√¢metros Configur√°veis
- **N√∫mero de bra√ßos (k)**: 2 a 50
- **N√∫mero de tentativas (T)**: 100 a 50.000
- **Epsilon (Œµ)**: 0.01 a 0.5 (para Epsilon-Greedy)
- **Alpha (Œ±) Prior**: 0.1 a 10.0 (para Thompson Sampling)
- **Beta (Œ≤) Prior**: 0.1 a 10.0 (para Thompson Sampling)
- **Learning Rate (Œ±)**: 0.01 a 1.0 (para Gradient Bandit)
- **Seed**: Para resultados reproduz√≠veis

### Visualiza√ß√µes Interativas
- Gr√°ficos de regret m√©dio ao longo do tempo
- Porcentagem de a√ß√µes √≥timas
- Distribui√ß√£o de recompensas por bra√ßo
- Distribui√ß√£o binomial negativa customiz√°vel
- Tabela detalhada com estat√≠sticas por bra√ßo

### Controles de Interface
- ‚úÖ Mostrar/ocultar detalhes dos bra√ßos
- ‚úÖ Mostrar/ocultar distribui√ß√£o binomial negativa
- ü§ñ Sele√ß√£o de algoritmo
- ‚öôÔ∏è Par√¢metros b√°sicos e avan√ßados
- ‚ÑπÔ∏è Informa√ß√µes sobre algoritmos

## üõ†Ô∏è Como Usar

### 1. Configura√ß√£o
1. Na barra lateral, selecione o algoritmo desejado
2. Ajuste o n√∫mero de bra√ßos e tentativas
3. Configure par√¢metros espec√≠ficos (epsilon para Epsilon-Greedy)
4. Defina uma seed para reprodutibilidade (opcional)

### 2. Execu√ß√£o
1. Clique no bot√£o "üöÄ Executar Simula√ß√£o"
2. Aguarde o processamento (pode levar alguns segundos para simula√ß√µes grandes)
3. Analise os resultados nos gr√°ficos e estat√≠sticas

### 3. An√°lise
- **Regret M√©dio**: Quanto menor, melhor o algoritmo
- **Taxa de Sucesso**: Porcentagem de vezes que escolheu o bra√ßo √≥timo
- **Detalhes dos Bra√ßos**: Estat√≠sticas individuais de cada op√ß√£o

## üìä M√©tricas Explicadas

### Regret
O regret mede a diferen√ßa entre a recompensa √≥tima e a recompensa obtida:
```
Regret(t) = (Recompensa_√ìtima_Acumulada - Recompensa_Obtida) / t
```

### Taxa de A√ß√µes √ìtimas
Porcentagem de vezes que o algoritmo escolheu o bra√ßo com maior recompensa m√©dia:
```
Taxa_Sucesso(t) = N√∫mero_A√ß√µes_√ìtimas / Total_A√ß√µes
```

## üéØ Casos de Uso

### Educacional
- Demonstrar conceitos de explora√ß√£o vs. explora√ß√£o
- Comparar performance de diferentes algoritmos
- Visualizar converg√™ncia e learning curves

### Pesquisa
- Testar configura√ß√µes de par√¢metros
- Avaliar robustez dos algoritmos
- An√°lise de sensibilidade

### Pr√°tico
- Entender trade-offs em problemas de decis√£o
- Modelar cen√°rios de A/B testing
- Otimiza√ß√£o de recomenda√ß√µes

## üîß Requisitos T√©cnicos

### Depend√™ncias
- Python 3.11+
- Streamlit
- NumPy
- Pandas
- Plotly
- SciPy
- Matplotlib

### Configura√ß√£o do Servidor
A aplica√ß√£o est√° configurada para rodar em:
- **Endere√ßo**: 0.0.0.0
- **Porta**: 5000
- **Modo**: Headless

## üìà Distribui√ß√µes de Recompensa

### Distribui√ß√£o Uniforme (Padr√£o)
Cada bra√ßo tem uma distribui√ß√£o uniforme U(a,b) onde:
- a e b s√£o gerados aleatoriamente
- Garantia: a ‚â§ b para cada bra√ßo
- Melhor bra√ßo: aquele com maior (a+b)/2

### Distribui√ß√£o Binomial Negativa (Adicional)
Visualiza√ß√£o educacional com par√¢metros configur√°veis:
- **r**: N√∫mero de falhas
- **p**: Probabilidade de sucesso
- **Uso**: Demonstra√ß√£o de outras distribui√ß√µes de probabilidade

## üé® Interface

### Layout Responsivo
- Sidebar com controles organizados
- √Årea principal com visualiza√ß√µes
- Colunas adapt√°veis para diferentes telas

### Elementos Interativos
- Sliders para par√¢metros num√©ricos
- Selectbox para escolhas categ√≥ricas
- Checkboxes para controles opcionais
- Bot√µes com a√ß√µes claras

### Feedback Visual
- Spinners durante processamento
- M√©tricas destacadas
- Gr√°ficos interativos com hover
- C√≥digos de cores consistentes

## üî¨ Detalhes de Implementa√ß√£o

### UCB1
```python
# Primeiras k rodadas: puxar cada bra√ßo uma vez
if t < k:
    kth = t
else:
    # UCB1: exploration bonus
    exploration_bonus = sqrt(2 * log(t + 1) / pulls)
    kth = argmax(estimates + exploration_bonus)
```

### Epsilon-Greedy
```python
# Explora√ß√£o vs. Explora√ß√£o
if random() < epsilon:
    kth = random_choice()  # Explorar
else:
    kth = argmax(estimates)  # Explorar
```

### Thompson Sampling
```python
# Amostrar de distribui√ß√µes Beta para cada bra√ßo
samples = [beta.rvs(alpha[i], beta[i]) for i in range(k)]
kth = argmax(samples)

# Atualizar distribui√ß√µes com recompensa normalizada
normalized_reward = (reward - a[i]) / (b[i] - a[i])
alpha[i] += normalized_reward
beta[i] += (1 - normalized_reward)
```

### Gradient Bandit
```python
# Calcular probabilidades softmax
probabilities = softmax(preferences)

# Selecionar a√ß√£o
kth = random_choice(k, p=probabilities)

# Atualizar prefer√™ncias com gradiente
for arm in range(k):
    if arm == kth:
        preferences[arm] += learning_rate * (reward - baseline) * (1 - probabilities[arm])
    else:
        preferences[arm] -= learning_rate * (reward - baseline) * probabilities[arm]
```

### Compara√ß√£o Justa
- Mesma seed para todos os algoritmos
- Ambiente id√™ntico (mesmas distribui√ß√µes a,b)
- M√©tricas calculadas de forma consistente
- Compara√ß√£o visual lado a lado

## üìù Cr√©ditos

Baseado no trabalho original de: [github.com/petroud/E-greedy_and_UCB_algorithms/](https://github.com/petroud/E-greedy_and_UCB_algorithms/)

## ü§ù Contribui√ß√£o

Este projeto √© educacional e est√° aberto para melhorias. Sugest√µes de novos algoritmos, visualiza√ß√µes ou funcionalidades s√£o bem-vindas.

---

*Desenvolvido com ‚ù§Ô∏è para demonstrar os conceitos fundamentais de Multi-Armed Bandit de forma interativa e educacional.*