import streamlit as st
import matplotlib.pyplot as plt
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from scipy.stats import nbinom
import pandas as pd

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Simula√ß√£o Multi-Armed Bandit",
    page_icon="üé∞",
    layout="wide"
)

# T√≠tulo principal
st.title("üé∞ Simula√ß√£o Interativa de Algoritmos Multi-Armed Bandit")
st.markdown("### Cr√©ditos: github.com/petroud/E-greedy_and_UCB_algorithms/")

# Sidebar para controles
st.sidebar.header("Par√¢metros de Simula√ß√£o")

# Sele√ß√£o do algoritmo
algorithm = st.sidebar.selectbox(
    "Algoritmo", 
    ["UCB1", "Epsilon-Greedy", "Compara√ß√£o"],
    help="Escolha o algoritmo para simula√ß√£o"
)

# Controles interativos
k = st.sidebar.slider("N√∫mero de bra√ßos (k)", min_value=2, max_value=20, value=10, help="N√∫mero de m√°quinas ca√ßa-n√≠queis")
T = st.sidebar.slider("N√∫mero de tentativas (T)", min_value=100, max_value=20000, value=10000, step=100, help="N√∫mero total de rodadas")

# Par√¢metro espec√≠fico para Epsilon-Greedy
epsilon = 0.1  # Valor padr√£o
if algorithm in ["Epsilon-Greedy", "Compara√ß√£o"]:
    epsilon = st.sidebar.slider("Epsilon (Œµ)", min_value=0.01, max_value=0.5, value=0.1, step=0.01, help="Taxa de explora√ß√£o para Epsilon-Greedy")

# Bot√£o para executar simula√ß√£o
run_simulation = st.sidebar.button("Executar Simula√ß√£o", type="primary")

# Seed para reprodutibilidade
seed = st.sidebar.number_input("Seed (opcional)", min_value=0, max_value=9999, value=42, help="Para resultados reproduz√≠veis")

def run_ucb1_simulation(k, T, seed=None):
    """Executa a simula√ß√£o do algoritmo UCB1"""
    if seed is not None:
        np.random.seed(seed)
    
    # Vetores do modelo
    ucb_pulls = np.zeros(k)
    ucb_estimate_M = np.zeros(k)
    ucb_total_rewards = np.zeros(k)
    ucb_inst_score = np.zeros(T)
    ucb_best_score = np.zeros(T)
    ucb_alg_score = np.zeros(T)
    ucb_regret = np.zeros(T)
    ucb_optimal_action = np.zeros(T)
    
    # Definindo distribui√ß√µes de probabilidade
    a = np.random.random(k)
    b = np.random.random(k)
    for i in range(k):
        if a[i] > b[i]:
            a[i], b[i] = b[i], a[i]
    
    mean = (a + b) / 2
    best = np.max(mean)
    index_best = np.where(mean == best)[0][0]
    
    def pull(i):
        """Puxa a alavanca i e retorna recompensa"""
        return np.random.uniform(a[i], b[i])
    
    success = 0
    
    def success_rate(optimal, i, total):
        """Calcula taxa de sucesso"""
        nonlocal success
        if i == index_best:
            success += 1
        optimal[total] = success / total if total > 0 else 0
    
    def update_stats_ucb(reward, i, t):
        """Atualiza estat√≠sticas do UCB"""
        ucb_pulls[i] += 1
        ucb_inst_score[t] = reward
        ucb_total_rewards[i] += reward
        ucb_best_score[t] = ucb_best_score[t-1] + best if t > 0 else best
        ucb_alg_score[t] = ucb_alg_score[t-1] + ucb_inst_score[t] if t > 0 else ucb_inst_score[t]
        ucb_estimate_M[i] = ucb_total_rewards[i] / ucb_pulls[i]
        ucb_regret[t] = (ucb_best_score[t] - ucb_alg_score[t]) / (t + 1)
    
    # Simula√ß√£o principal
    for t in range(T):
        if t == 0:
            # Primeira rodada: escolhe bra√ßo aleat√≥rio
            kth = np.random.randint(0, k)
        else:
            # UCB1: exploration bonus
            exploration_bonus = np.sqrt(np.log(t) / (ucb_pulls + 0.0001))
            kth = np.argmax(ucb_estimate_M + exploration_bonus)
        
        reward = pull(kth)
        update_stats_ucb(reward, kth, t)
        success_rate(ucb_optimal_action, kth, t)
    
    return {
        'regret': ucb_regret,
        'optimal_action': ucb_optimal_action,
        'mean_rewards': mean,
        'best_arm': index_best,
        'pulls': ucb_pulls,
        'total_rewards': ucb_total_rewards,
        'final_regret': ucb_regret[-1],
        'final_success_rate': ucb_optimal_action[-1],
        'a': a,
        'b': b
    }

def run_epsilon_greedy_simulation(k, T, epsilon, seed=None):
    """Executa a simula√ß√£o do algoritmo Epsilon-Greedy"""
    if seed is not None:
        np.random.seed(seed)
    
    # Vetores do modelo
    eg_pulls = np.zeros(k)
    eg_estimate_M = np.zeros(k)
    eg_total_rewards = np.zeros(k)
    eg_inst_score = np.zeros(T)
    eg_best_score = np.zeros(T)
    eg_alg_score = np.zeros(T)
    eg_regret = np.zeros(T)
    eg_optimal_action = np.zeros(T)
    
    # Definindo distribui√ß√µes de probabilidade (mesmas do UCB para compara√ß√£o justa)
    a = np.random.random(k)
    b = np.random.random(k)
    for i in range(k):
        if a[i] > b[i]:
            a[i], b[i] = b[i], a[i]
    
    mean = (a + b) / 2
    best = np.max(mean)
    index_best = np.where(mean == best)[0][0]
    
    def pull(i):
        """Puxa a alavanca i e retorna recompensa"""
        return np.random.uniform(a[i], b[i])
    
    success = 0
    
    def success_rate(optimal, i, total):
        """Calcula taxa de sucesso"""
        nonlocal success
        if i == index_best:
            success += 1
        optimal[total] = success / total if total > 0 else 0
    
    def update_stats_eg(reward, i, t):
        """Atualiza estat√≠sticas do Epsilon-Greedy"""
        eg_pulls[i] += 1
        eg_inst_score[t] = reward
        eg_total_rewards[i] += reward
        eg_best_score[t] = eg_best_score[t-1] + best if t > 0 else best
        eg_alg_score[t] = eg_alg_score[t-1] + eg_inst_score[t] if t > 0 else eg_inst_score[t]
        eg_estimate_M[i] = eg_total_rewards[i] / eg_pulls[i]
        eg_regret[t] = (eg_best_score[t] - eg_alg_score[t]) / (t + 1)
    
    # Simula√ß√£o principal
    for t in range(T):
        if t == 0:
            # Primeira rodada: escolhe bra√ßo aleat√≥rio
            kth = np.random.randint(0, k)
        else:
            # Epsilon-Greedy: explorar vs exploitar
            if np.random.random() < epsilon:
                # Explorar: escolher bra√ßo aleat√≥rio
                kth = np.random.randint(0, k)
            else:
                # Exploitar: escolher melhor bra√ßo conhecido
                # Evitar bra√ßos n√£o testados usando um valor baixo
                estimates = np.copy(eg_estimate_M)
                estimates[eg_pulls == 0] = 0  # Bra√ßos n√£o testados t√™m estimativa 0
                kth = np.argmax(estimates)
        
        reward = pull(kth)
        update_stats_eg(reward, kth, t)
        success_rate(eg_optimal_action, kth, t)
    
    return {
        'regret': eg_regret,
        'optimal_action': eg_optimal_action,
        'mean_rewards': mean,
        'best_arm': index_best,
        'pulls': eg_pulls,
        'total_rewards': eg_total_rewards,
        'final_regret': eg_regret[-1],
        'final_success_rate': eg_optimal_action[-1],
        'a': a,
        'b': b
    }

# Execu√ß√£o da simula√ß√£o
if run_simulation or 'simulation_results' not in st.session_state:
    if algorithm == "UCB1":
        with st.spinner('Executando simula√ß√£o UCB1...'):
            results = run_ucb1_simulation(k, T, seed)
            st.session_state.simulation_results = results
            st.session_state.algorithm = algorithm
    
    elif algorithm == "Epsilon-Greedy":
        with st.spinner('Executando simula√ß√£o Epsilon-Greedy...'):
            results = run_epsilon_greedy_simulation(k, T, epsilon, seed)
            st.session_state.simulation_results = results
            st.session_state.algorithm = algorithm
    
    elif algorithm == "Compara√ß√£o":
        with st.spinner('Executando compara√ß√£o de algoritmos...'):
            # Executar ambos os algoritmos com a mesma seed para compara√ß√£o justa
            ucb_results = run_ucb1_simulation(k, T, seed)
            eg_results = run_epsilon_greedy_simulation(k, T, epsilon, seed)
            
            # Armazenar resultados de ambos
            st.session_state.simulation_results = {
                'ucb': ucb_results,
                'epsilon_greedy': eg_results
            }
            st.session_state.algorithm = algorithm
    
    st.session_state.k = k
    st.session_state.T = T
    if algorithm in ["Epsilon-Greedy", "Compara√ß√£o"]:
        st.session_state.epsilon = epsilon

# Verificar se h√° resultados para mostrar
if 'simulation_results' in st.session_state:
    results = st.session_state.simulation_results
    current_algorithm = st.session_state.get('algorithm', 'UCB1')
    
    if current_algorithm == "Compara√ß√£o":
        # Modo de compara√ß√£o
        ucb_results = results['ucb']
        eg_results = results['epsilon_greedy']
        
        # Estat√≠sticas principais - Compara√ß√£o
        st.header("üìä Estat√≠sticas da Compara√ß√£o")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("UCB1")
            st.metric("Melhor Bra√ßo", f"#{ucb_results['best_arm']}")
            st.metric("Taxa de Sucesso Final", f"{ucb_results['final_success_rate']:.2%}")
            st.metric("Regret Final", f"{ucb_results['final_regret']:.4f}")
            best_reward_ucb = ucb_results['mean_rewards'][ucb_results['best_arm']]
            st.metric("Recompensa M√©dia do Melhor Bra√ßo", f"{best_reward_ucb:.4f}")
        
        with col2:
            st.subheader(f"Epsilon-Greedy (Œµ={st.session_state.get('epsilon', 0.1)})")
            st.metric("Melhor Bra√ßo", f"#{eg_results['best_arm']}")
            st.metric("Taxa de Sucesso Final", f"{eg_results['final_success_rate']:.2%}")
            st.metric("Regret Final", f"{eg_results['final_regret']:.4f}")
            best_reward_eg = eg_results['mean_rewards'][eg_results['best_arm']]
            st.metric("Recompensa M√©dia do Melhor Bra√ßo", f"{best_reward_eg:.4f}")
        
        # Gr√°ficos de compara√ß√£o
        st.header("üìà Visualiza√ß√µes de Compara√ß√£o")
        
        col_left, col_right = st.columns(2)
        
        with col_left:
            st.subheader("Regret ao Longo do Tempo - Compara√ß√£o")
            
            fig_regret = go.Figure()
            fig_regret.add_trace(go.Scatter(
                x=np.arange(1, st.session_state.T + 1),
                y=ucb_results['regret'],
                mode='lines',
                name='UCB1',
                line=dict(color='red', width=2)
            ))
            fig_regret.add_trace(go.Scatter(
                x=np.arange(1, st.session_state.T + 1),
                y=eg_results['regret'],
                mode='lines',
                name=f'Epsilon-Greedy (Œµ={st.session_state.get("epsilon", 0.1)})',
                line=dict(color='green', width=2)
            ))
            
            fig_regret.update_layout(
                title=f"Compara√ß√£o de Regret - T={st.session_state.T}, k={st.session_state.k}",
                xaxis_title="Rodada T",
                yaxis_title="Regret Total",
                hovermode='x'
            )
            
            st.plotly_chart(fig_regret, use_container_width=True)
        
        with col_right:
            st.subheader("A√ß√µes √ìtimas - Compara√ß√£o")
            
            fig_optimal = go.Figure()
            fig_optimal.add_trace(go.Scatter(
                x=np.arange(1, st.session_state.T + 1),
                y=ucb_results['optimal_action'] * 100,
                mode='lines',
                name='UCB1',
                line=dict(color='red', width=2)
            ))
            fig_optimal.add_trace(go.Scatter(
                x=np.arange(1, st.session_state.T + 1),
                y=eg_results['optimal_action'] * 100,
                mode='lines',
                name=f'Epsilon-Greedy (Œµ={st.session_state.get("epsilon", 0.1)})',
                line=dict(color='green', width=2)
            ))
            
            fig_optimal.update_layout(
                title=f"Compara√ß√£o de A√ß√µes √ìtimas - T={st.session_state.T}, k={st.session_state.k}",
                xaxis_title="Rodada T",
                yaxis_title="% A√ß√£o √≥tima tomada",
                hovermode='x'
            )
            
            st.plotly_chart(fig_optimal, use_container_width=True)
        
        # Usar os resultados do UCB para gr√°ficos adicionais (bra√ßos e distribui√ß√µes)
        results_for_arms = ucb_results
        
    else:
        # Modo de algoritmo √∫nico
        algorithm_name = current_algorithm
        color = 'red' if current_algorithm == 'UCB1' else 'green'
        
        # Estat√≠sticas principais
        st.header(f"üìä Estat√≠sticas da Simula√ß√£o - {algorithm_name}")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Melhor Bra√ßo", f"#{results['best_arm']}")
        
        with col2:
            st.metric("Taxa de Sucesso Final", f"{results['final_success_rate']:.2%}")
        
        with col3:
            st.metric("Regret Final", f"{results['final_regret']:.4f}")
        
        with col4:
            best_reward = results['mean_rewards'][results['best_arm']]
            st.metric("Recompensa M√©dia do Melhor Bra√ßo", f"{best_reward:.4f}")
        
        # Gr√°ficos principais
        st.header("üìà Visualiza√ß√µes")
        
        col_left, col_right = st.columns(2)
        
        with col_left:
            st.subheader("Regret ao Longo do Tempo")
            
            fig_regret = go.Figure()
            fig_regret.add_trace(go.Scatter(
                x=np.arange(1, st.session_state.T + 1),
                y=results['regret'],
                mode='lines',
                name=algorithm_name,
                line=dict(color=color, width=2)
            ))
            
            fig_regret.update_layout(
                title=f"Regret para T={st.session_state.T} rodadas e k={st.session_state.k} bandits",
                xaxis_title="Rodada T",
                yaxis_title="Regret Total",
                hovermode='x'
            )
            
            st.plotly_chart(fig_regret, use_container_width=True)
        
        with col_right:
            st.subheader("Porcentagem de A√ß√µes √ìtimas")
            
            fig_optimal = go.Figure()
            fig_optimal.add_trace(go.Scatter(
                x=np.arange(1, st.session_state.T + 1),
                y=results['optimal_action'] * 100,
                mode='lines',
                name=algorithm_name,
                line=dict(color=color, width=2)
            ))
            
            fig_optimal.update_layout(
                title=f"A√ß√£o √≥tima tomada para T={st.session_state.T} rodadas e k={st.session_state.k} bandits",
                xaxis_title="Rodada T",
                yaxis_title="% A√ß√£o √≥tima tomada",
                hovermode='x'
            )
            
            st.plotly_chart(fig_optimal, use_container_width=True)
        
        results_for_arms = results
    
    # Gr√°fico da distribui√ß√£o de recompensas dos bra√ßos
    st.subheader("Distribui√ß√£o de Recompensas por Bra√ßo")
    
    # Criar DataFrame para melhor visualiza√ß√£o
    arms_data = pd.DataFrame({
        'Bra√ßo': [f'Bra√ßo {i}' for i in range(st.session_state.k)],
        'Recompensa M√©dia': results_for_arms['mean_rewards'],
        'N√∫mero de Puxadas': results_for_arms['pulls'],
        'Recompensa Total': results_for_arms['total_rewards'],
        '√â o Melhor': ['Sim' if i == results_for_arms['best_arm'] else 'N√£o' for i in range(st.session_state.k)]
    })
    
    # Gr√°fico de barras das recompensas m√©dias
    fig_arms = px.bar(
        arms_data, 
        x='Bra√ßo', 
        y='Recompensa M√©dia',
        color='√â o Melhor',
        color_discrete_map={'Sim': 'gold', 'N√£o': 'lightblue'},
        title="Recompensa M√©dia por Bra√ßo"
    )
    
    fig_arms.update_layout(
        xaxis_title="Bra√ßos",
        yaxis_title="Recompensa M√©dia",
        showlegend=True
    )
    
    st.plotly_chart(fig_arms, use_container_width=True)
    
    # Distribui√ß√£o Binomial Negativa
    st.subheader("Distribui√ß√£o Binomial Negativa")
    
    # Par√¢metros para a distribui√ß√£o binomial negativa
    st.sidebar.subheader("Par√¢metros Distribui√ß√£o Binomial Negativa")
    n_failures = st.sidebar.slider("N√∫mero de falhas (r)", min_value=1, max_value=50, value=10)
    prob_success = st.sidebar.slider("Probabilidade de sucesso (p)", min_value=0.01, max_value=0.99, value=0.3, step=0.01)
    
    # Gerar distribui√ß√£o binomial negativa
    x_values = np.arange(0, 100)
    nb_pmf = nbinom.pmf(x_values, n_failures, prob_success)
    
    fig_nb = go.Figure()
    fig_nb.add_trace(go.Scatter(
        x=x_values,
        y=nb_pmf,
        mode='lines+markers',
        name=f'Binomial Negativa (r={n_failures}, p={prob_success})',
        line=dict(color='green', width=2),
        marker=dict(size=4)
    ))
    
    fig_nb.update_layout(
        title=f"Distribui√ß√£o Binomial Negativa (r={n_failures}, p={prob_success})",
        xaxis_title="N√∫mero de sucessos",
        yaxis_title="Probabilidade",
        hovermode='x'
    )
    
    st.plotly_chart(fig_nb, use_container_width=True)
    
    # Tabela detalhada dos bra√ßos
    st.subheader("Detalhes dos Bra√ßos")
    st.dataframe(arms_data, use_container_width=True)
    
    # Informa√ß√µes adicionais sobre a distribui√ß√£o binomial negativa
    st.subheader("Informa√ß√µes sobre a Distribui√ß√£o Binomial Negativa")
    st.write(f"""
    A distribui√ß√£o binomial negativa modela o n√∫mero de sucessos em uma sequ√™ncia de tentativas independentes antes de obter um n√∫mero fixo de falhas.
    
    **Par√¢metros atuais:**
    - **r (n√∫mero de falhas):** {n_failures}
    - **p (probabilidade de sucesso):** {prob_success}
    
    **Estat√≠sticas:**
    - **M√©dia:** {n_failures * (1 - prob_success) / prob_success:.2f}
    - **Vari√¢ncia:** {n_failures * (1 - prob_success) / (prob_success ** 2):.2f}
    """)

else:
    st.info("üëÜ Configure os par√¢metros na barra lateral e clique em 'Executar Simula√ß√£o' para come√ßar!")
    
    # Explica√ß√£o do algoritmo
    st.header("‚ÑπÔ∏è Sobre o Algoritmo UCB1")
    st.write("""
    O **Upper Confidence Bound (UCB1)** √© um algoritmo para o problema do multi-armed bandit que equilibra explora√ß√£o e explora√ß√£o.
    
    **Como funciona:**
    1. **Explora√ß√£o vs Explora√ß√£o:** O algoritmo escolhe o bra√ßo que maximiza a recompensa estimada mais um b√¥nus de explora√ß√£o
    2. **F√≥rmula UCB1:** Escolhe o bra√ßo i que maximiza: Œº·µ¢ + ‚àö(2ln(t)/n·µ¢)
       - Œº·µ¢: recompensa m√©dia estimada do bra√ßo i
       - t: n√∫mero total de rodadas
       - n·µ¢: n√∫mero de vezes que o bra√ßo i foi escolhido
    3. **B√¥nus de Explora√ß√£o:** Diminui conforme o bra√ßo √© mais explorado, incentivando a explora√ß√£o de bra√ßos menos testados
    
    **M√©tricas Importantes:**
    - **Regret:** Diferen√ßa entre a recompensa √≥tima e a recompensa obtida
    - **Taxa de A√ß√µes √ìtimas:** Porcentagem de vezes que o melhor bra√ßo foi escolhido
    """)
