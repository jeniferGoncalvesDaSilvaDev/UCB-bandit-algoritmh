import streamlit as st
import matplotlib.pyplot as plt
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from scipy.stats import nbinom
import pandas as pd

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Simula√ß√£o Multi-Armed Bandit",
    page_icon="üé∞",
    layout="wide"
)

# T√≠tulo principal
st.title("üé∞ Simula√ß√£o Interativa de Algoritmos Multi-Armed Bandit")

# Sidebar para controles
st.sidebar.header("üéØ Configura√ß√µes da Simula√ß√£o")

# Sele√ß√£o do algoritmo
algorithm = st.sidebar.selectbox(
    "ü§ñ Algoritmo", 
    ["UCB1", "Epsilon-Greedy", "Compara√ß√£o"],
    help="Escolha o algoritmo para simula√ß√£o"
)

# Se√ß√£o de par√¢metros b√°sicos
st.sidebar.subheader("‚öôÔ∏è Par√¢metros B√°sicos")
k = st.sidebar.slider("N√∫mero de bra√ßos (k)", min_value=2, max_value=50, value=10, help="N√∫mero de m√°quinas ca√ßa-n√≠queis")
T = st.sidebar.slider("N√∫mero de tentativas (T)", min_value=100, max_value=50000, value=10000, step=100, help="N√∫mero total de rodadas")

# Par√¢metro espec√≠fico para Epsilon-Greedy
epsilon = 0.1  # Valor padr√£o
if algorithm in ["Epsilon-Greedy", "Compara√ß√£o"]:
    st.sidebar.subheader("üéØ Par√¢metros Epsilon-Greedy")
    epsilon = st.sidebar.slider("Epsilon (Œµ)", min_value=0.01, max_value=0.5, value=0.1, step=0.01, help="Taxa de explora√ß√£o para Epsilon-Greedy")

# Op√ß√µes avan√ßadas
st.sidebar.subheader("üîß Op√ß√µes Avan√ßadas")
seed = st.sidebar.number_input("Seed (opcional)", min_value=0, max_value=9999, value=42, help="Para resultados reproduz√≠veis")

# Personaliza√ß√£o de exibi√ß√£o
show_arm_details = st.sidebar.checkbox("Mostrar detalhes dos bra√ßos", value=True, help="Exibir tabela com estat√≠sticas detalhadas")
show_binomial = st.sidebar.checkbox("Mostrar distribui√ß√£o binomial negativa", value=True, help="Exibir gr√°fico da distribui√ß√£o binomial negativa")

# Bot√£o para executar simula√ß√£o
st.sidebar.markdown("---")
run_simulation = st.sidebar.button("üöÄ Executar Simula√ß√£o", type="primary")

# Informa√ß√µes sobre a simula√ß√£o
if st.sidebar.button("‚ÑπÔ∏è Sobre os Algoritmos"):
    st.sidebar.info("""
    **UCB1**: Usa confian√ßa superior para balancear explora√ß√£o e explora√ß√£o.
    
    **Epsilon-Greedy**: Explora aleatoriamente com probabilidade Œµ, sen√£o explora o melhor bra√ßo conhecido.
    
    **Compara√ß√£o**: Executa ambos algoritmos simultaneamente para compara√ß√£o.
    """)

def run_ucb1_simulation(k, T, seed=None):
    """Executa a simula√ß√£o do algoritmo UCB1"""
    if seed is not None:
        np.random.seed(seed)
    
    # Vetores do modelo
    ucb_pulls = np.zeros(k)
    ucb_estimate_M = np.zeros(k)
    ucb_total_rewards = np.zeros(k)
    ucb_inst_score = np.zeros(T)
    ucb_best_score = np.zeros(T)
    ucb_alg_score = np.zeros(T)
    ucb_regret = np.zeros(T)
    ucb_optimal_action = np.zeros(T)
    
    # Definindo distribui√ß√µes de probabilidade
    a = np.random.random(k)
    b = np.random.random(k)
    for i in range(k):
        if a[i] > b[i]:
            a[i], b[i] = b[i], a[i]
    
    mean = (a + b) / 2
    best = np.max(mean)
    index_best = np.where(mean == best)[0][0]
    
    def pull(i):
        """Puxa a alavanca i e retorna recompensa"""
        return np.random.uniform(a[i], b[i])
    
    success = 0
    
    def success_rate(optimal, i, total):
        """Calcula taxa de sucesso"""
        nonlocal success
        if i == index_best:
            success += 1
        optimal[total] = success / (total + 1)
    
    def update_stats_ucb(reward, i, t):
        """Atualiza estat√≠sticas do UCB"""
        ucb_pulls[i] += 1
        ucb_inst_score[t] = reward
        ucb_total_rewards[i] += reward
        ucb_best_score[t] = ucb_best_score[t-1] + best if t > 0 else best
        ucb_alg_score[t] = ucb_alg_score[t-1] + ucb_inst_score[t] if t > 0 else ucb_inst_score[t]
        ucb_estimate_M[i] = ucb_total_rewards[i] / ucb_pulls[i]
        ucb_regret[t] = (ucb_best_score[t] - ucb_alg_score[t]) / (t + 1)
    
    # Simula√ß√£o principal
    for t in range(T):
        if t < k:
            # Primeiras k rodadas: puxar cada bra√ßo uma vez
            kth = t
        else:
            # UCB1: exploration bonus com f√≥rmula correta
            exploration_bonus = np.sqrt(2 * np.log(t + 1) / ucb_pulls)
            kth = np.argmax(ucb_estimate_M + exploration_bonus)
        
        reward = pull(kth)
        update_stats_ucb(reward, kth, t)
        success_rate(ucb_optimal_action, kth, t)
    
    return {
        'regret': ucb_regret,
        'optimal_action': ucb_optimal_action,
        'mean_rewards': mean,
        'best_arm': index_best,
        'pulls': ucb_pulls,
        'total_rewards': ucb_total_rewards,
        'final_regret': ucb_regret[-1],
        'final_success_rate': ucb_optimal_action[-1],
        'a': a,
        'b': b
    }

def run_epsilon_greedy_simulation(k, T, epsilon, seed=None):
    """Executa a simula√ß√£o do algoritmo Epsilon-Greedy"""
    if seed is not None:
        np.random.seed(seed)
    
    # Vetores do modelo
    eg_pulls = np.zeros(k)
    eg_estimate_M = np.zeros(k)
    eg_total_rewards = np.zeros(k)
    eg_inst_score = np.zeros(T)
    eg_best_score = np.zeros(T)
    eg_alg_score = np.zeros(T)
    eg_regret = np.zeros(T)
    eg_optimal_action = np.zeros(T)
    
    # Definindo distribui√ß√µes de probabilidade (mesmas do UCB para compara√ß√£o justa)
    a = np.random.random(k)
    b = np.random.random(k)
    for i in range(k):
        if a[i] > b[i]:
            a[i], b[i] = b[i], a[i]
    
    mean = (a + b) / 2
    best = np.max(mean)
    index_best = np.where(mean == best)[0][0]
    
    def pull(i):
        """Puxa a alavanca i e retorna recompensa"""
        return np.random.uniform(a[i], b[i])
    
    success = 0
    
    def success_rate(optimal, i, total):
        """Calcula taxa de sucesso"""
        nonlocal success
        if i == index_best:
            success += 1
        optimal[total] = success / (total + 1)
    
    def update_stats_eg(reward, i, t):
        """Atualiza estat√≠sticas do Epsilon-Greedy"""
        eg_pulls[i] += 1
        eg_inst_score[t] = reward
        eg_total_rewards[i] += reward
        eg_best_score[t] = eg_best_score[t-1] + best if t > 0 else best
        eg_alg_score[t] = eg_alg_score[t-1] + eg_inst_score[t] if t > 0 else eg_inst_score[t]
        eg_estimate_M[i] = eg_total_rewards[i] / eg_pulls[i]
        eg_regret[t] = (eg_best_score[t] - eg_alg_score[t]) / (t + 1)
    
    # Simula√ß√£o principal
    for t in range(T):
        # Epsilon-Greedy: explorar vs exploitar
        if np.random.random() < epsilon:
            # Explorar: escolher bra√ßo aleat√≥rio
            kth = np.random.randint(0, k)
        else:
            # Exploitar: escolher melhor bra√ßo conhecido
            # Bra√ßos n√£o testados t√™m estimativa 0 devido √† inicializa√ß√£o
            estimates = np.copy(eg_estimate_M)
            estimates[eg_pulls == 0] = 0  # Bra√ßos n√£o testados t√™m estimativa 0
            kth = np.argmax(estimates)
        
        reward = pull(kth)
        update_stats_eg(reward, kth, t)
        success_rate(eg_optimal_action, kth, t)
    
    return {
        'regret': eg_regret,
        'optimal_action': eg_optimal_action,
        'mean_rewards': mean,
        'best_arm': index_best,
        'pulls': eg_pulls,
        'total_rewards': eg_total_rewards,
        'final_regret': eg_regret[-1],
        'final_success_rate': eg_optimal_action[-1],
        'a': a,
        'b': b
    }

# Execu√ß√£o da simula√ß√£o
if run_simulation:
    if algorithm == "UCB1":
        with st.spinner('Executando simula√ß√£o UCB1...'):
            results = run_ucb1_simulation(k, T, seed)
            st.session_state.simulation_results = results
            st.session_state.algorithm = algorithm
    
    elif algorithm == "Epsilon-Greedy":
        with st.spinner('Executando simula√ß√£o Epsilon-Greedy...'):
            results = run_epsilon_greedy_simulation(k, T, epsilon, seed)
            st.session_state.simulation_results = results
            st.session_state.algorithm = algorithm
    
    elif algorithm == "Compara√ß√£o":
        with st.spinner('Executando compara√ß√£o de algoritmos...'):
            # Executar ambos os algoritmos com a mesma seed para compara√ß√£o justa
            ucb_results = run_ucb1_simulation(k, T, seed)
            eg_results = run_epsilon_greedy_simulation(k, T, epsilon, seed)
            
            # Armazenar resultados de ambos
            st.session_state.simulation_results = {
                'ucb': ucb_results,
                'epsilon_greedy': eg_results
            }
            st.session_state.algorithm = algorithm
    
    st.session_state.k = k
    st.session_state.T = T
    if algorithm in ["Epsilon-Greedy", "Compara√ß√£o"]:
        st.session_state.epsilon = epsilon

# Verificar se h√° resultados para mostrar
if 'simulation_results' in st.session_state:
    results = st.session_state.simulation_results
    current_algorithm = st.session_state.get('algorithm', 'UCB1')
    
    if current_algorithm == "Compara√ß√£o":
        # Modo de compara√ß√£o
        ucb_results = results['ucb']
        eg_results = results['epsilon_greedy']
        
        # Estat√≠sticas principais - Compara√ß√£o
        st.header("üìä Estat√≠sticas da Compara√ß√£o")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("UCB1")
            st.metric("Melhor Bra√ßo", f"#{ucb_results['best_arm']}")
            st.metric("Taxa de Sucesso Final", f"{ucb_results['final_success_rate']:.2%}")
            st.metric("Regret Final", f"{ucb_results['final_regret']:.4f}")
            best_reward_ucb = ucb_results['mean_rewards'][ucb_results['best_arm']]
            st.metric("Recompensa M√©dia do Melhor Bra√ßo", f"{best_reward_ucb:.4f}")
        
        with col2:
            st.subheader(f"Epsilon-Greedy (Œµ={st.session_state.get('epsilon', 0.1)})")
            st.metric("Melhor Bra√ßo", f"#{eg_results['best_arm']}")
            st.metric("Taxa de Sucesso Final", f"{eg_results['final_success_rate']:.2%}")
            st.metric("Regret Final", f"{eg_results['final_regret']:.4f}")
            best_reward_eg = eg_results['mean_rewards'][eg_results['best_arm']]
            st.metric("Recompensa M√©dia do Melhor Bra√ßo", f"{best_reward_eg:.4f}")
        
        # Gr√°ficos de compara√ß√£o
        st.header("üìà Visualiza√ß√µes de Compara√ß√£o")
        
        col_left, col_right = st.columns(2)
        
        with col_left:
            st.subheader("Regret ao Longo do Tempo - Compara√ß√£o")
            
            fig_regret = go.Figure()
            fig_regret.add_trace(go.Scatter(
                x=np.arange(1, st.session_state.T + 1),
                y=ucb_results['regret'],
                mode='lines',
                name='UCB1',
                line=dict(color='red', width=2)
            ))
            fig_regret.add_trace(go.Scatter(
                x=np.arange(1, st.session_state.T + 1),
                y=eg_results['regret'],
                mode='lines',
                name=f'Epsilon-Greedy (Œµ={st.session_state.get("epsilon", 0.1)})',
                line=dict(color='green', width=2)
            ))
            
            fig_regret.update_layout(
                title=f"Compara√ß√£o de Regret - T={st.session_state.T}, k={st.session_state.k}",
                xaxis_title="Rodada T",
                yaxis_title="Regret M√©dio",
                hovermode='x'
            )
            
            st.plotly_chart(fig_regret, use_container_width=True)
        
        with col_right:
            st.subheader("A√ß√µes √ìtimas - Compara√ß√£o")
            
            fig_optimal = go.Figure()
            fig_optimal.add_trace(go.Scatter(
                x=np.arange(1, st.session_state.T + 1),
                y=ucb_results['optimal_action'] * 100,
                mode='lines',
                name='UCB1',
                line=dict(color='red', width=2)
            ))
            fig_optimal.add_trace(go.Scatter(
                x=np.arange(1, st.session_state.T + 1),
                y=eg_results['optimal_action'] * 100,
                mode='lines',
                name=f'Epsilon-Greedy (Œµ={st.session_state.get("epsilon", 0.1)})',
                line=dict(color='green', width=2)
            ))
            
            fig_optimal.update_layout(
                title=f"Compara√ß√£o de A√ß√µes √ìtimas - T={st.session_state.T}, k={st.session_state.k}",
                xaxis_title="Rodada T",
                yaxis_title="% A√ß√£o √≥tima tomada",
                hovermode='x'
            )
            
            st.plotly_chart(fig_optimal, use_container_width=True)
        
        # Usar os resultados do UCB para gr√°ficos adicionais (bra√ßos e distribui√ß√µes)
        results_for_arms = ucb_results
        
    else:
        # Modo de algoritmo √∫nico
        algorithm_name = current_algorithm
        color = 'red' if current_algorithm == 'UCB1' else 'green'
        
        # Estat√≠sticas principais
        st.header(f"üìä Estat√≠sticas da Simula√ß√£o - {algorithm_name}")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Melhor Bra√ßo", f"#{results['best_arm']}")
        
        with col2:
            st.metric("Taxa de Sucesso Final", f"{results['final_success_rate']:.2%}")
        
        with col3:
            st.metric("Regret Final", f"{results['final_regret']:.4f}")
        
        with col4:
            best_reward = results['mean_rewards'][results['best_arm']]
            st.metric("Recompensa M√©dia do Melhor Bra√ßo", f"{best_reward:.4f}")
        
        # Gr√°ficos principais
        st.header("üìà Visualiza√ß√µes")
        
        col_left, col_right = st.columns(2)
        
        with col_left:
            st.subheader("Regret ao Longo do Tempo")
            
            fig_regret = go.Figure()
            fig_regret.add_trace(go.Scatter(
                x=np.arange(1, st.session_state.T + 1),
                y=results['regret'],
                mode='lines',
                name=algorithm_name,
                line=dict(color=color, width=2)
            ))
            
            fig_regret.update_layout(
                title=f"Regret para T={st.session_state.T} rodadas e k={st.session_state.k} bandits",
                xaxis_title="Rodada T",
                yaxis_title="Regret M√©dio",
                hovermode='x'
            )
            
            st.plotly_chart(fig_regret, use_container_width=True)
        
        with col_right:
            st.subheader("Porcentagem de A√ß√µes √ìtimas")
            
            fig_optimal = go.Figure()
            fig_optimal.add_trace(go.Scatter(
                x=np.arange(1, st.session_state.T + 1),
                y=results['optimal_action'] * 100,
                mode='lines',
                name=algorithm_name,
                line=dict(color=color, width=2)
            ))
            
            fig_optimal.update_layout(
                title=f"A√ß√£o √≥tima tomada para T={st.session_state.T} rodadas e k={st.session_state.k} bandits",
                xaxis_title="Rodada T",
                yaxis_title="% A√ß√£o √≥tima tomada",
                hovermode='x'
            )
            
            st.plotly_chart(fig_optimal, use_container_width=True)
        
        results_for_arms = results
    
    # Gr√°fico da distribui√ß√£o de recompensas dos bra√ßos (condicional)
    if show_arm_details:
        st.subheader("Distribui√ß√£o de Recompensas por Bra√ßo")
        
        # Criar DataFrame para melhor visualiza√ß√£o
        arms_data = pd.DataFrame({
            'Bra√ßo': [f'Bra√ßo {i}' for i in range(st.session_state.k)],
            'Recompensa M√©dia': results_for_arms['mean_rewards'],
            'N√∫mero de Puxadas': results_for_arms['pulls'],
            'Recompensa Total': results_for_arms['total_rewards'],
            '√â o Melhor': ['Sim' if i == results_for_arms['best_arm'] else 'N√£o' for i in range(st.session_state.k)]
        })
        
        # Gr√°fico de barras das recompensas m√©dias
        fig_arms = px.bar(
            arms_data, 
            x='Bra√ßo', 
            y='Recompensa M√©dia',
            color='√â o Melhor',
            color_discrete_map={'Sim': 'gold', 'N√£o': 'lightblue'},
            title="Recompensa M√©dia por Bra√ßo"
        )
        
        fig_arms.update_layout(
            xaxis_title="Bra√ßos",
            yaxis_title="Recompensa M√©dia",
            showlegend=True
        )
        
        st.plotly_chart(fig_arms, use_container_width=True)
        
        # Tabela detalhada dos bra√ßos
        st.subheader("Detalhes dos Bra√ßos")
        st.dataframe(arms_data, use_container_width=True)
    
    # Distribui√ß√£o Binomial Negativa (condicional)
    if show_binomial:
        st.subheader("Distribui√ß√£o Binomial Negativa")
        
        # Par√¢metros para a distribui√ß√£o binomial negativa
        if not hasattr(st.session_state, 'nb_params_initialized'):
            st.session_state.nb_params_initialized = True
            
        col1, col2 = st.columns(2)
        with col1:
            n_failures = st.slider("N√∫mero de falhas (r)", min_value=1, max_value=50, value=10)
        with col2:
            prob_success = st.slider("Probabilidade de sucesso (p)", min_value=0.01, max_value=0.99, value=0.3, step=0.01)
        
        # Gerar distribui√ß√£o binomial negativa
        x_values = np.arange(0, 100)
        nb_pmf = nbinom.pmf(x_values, n_failures, prob_success)
        
        fig_nb = go.Figure()
        fig_nb.add_trace(go.Scatter(
            x=x_values,
            y=nb_pmf,
            mode='lines+markers',
            name=f'Binomial Negativa (r={n_failures}, p={prob_success})',
            line=dict(color='green', width=2),
            marker=dict(size=4)
        ))
        
        fig_nb.update_layout(
            title=f"Distribui√ß√£o Binomial Negativa (r={n_failures}, p={prob_success})",
            xaxis_title="N√∫mero de sucessos",
            yaxis_title="Probabilidade",
            hovermode='x'
        )
        
        st.plotly_chart(fig_nb, use_container_width=True)
        
        # Informa√ß√µes adicionais sobre a distribui√ß√£o binomial negativa
        st.subheader("Informa√ß√µes sobre a Distribui√ß√£o Binomial Negativa")
        st.write(f"""
        A distribui√ß√£o binomial negativa modela o n√∫mero de sucessos em uma sequ√™ncia de tentativas independentes antes de obter um n√∫mero fixo de falhas.
        
        **Par√¢metros atuais:**
        - **r (n√∫mero de falhas):** {n_failures}
        - **p (probabilidade de sucesso):** {prob_success}
        
        **Estat√≠sticas:**
        - **M√©dia:** {n_failures * (1 - prob_success) / prob_success:.2f}
        - **Vari√¢ncia:** {n_failures * (1 - prob_success) / (prob_success ** 2):.2f}
        """)

else:
    st.info("üëÜ Configure os par√¢metros na barra lateral e clique em 'Executar Simula√ß√£o' para come√ßar!")
    
    # Explica√ß√£o do algoritmo
    st.header("‚ÑπÔ∏è Sobre o Algoritmo UCB1")
    st.write("""
    O **Upper Confidence Bound (UCB1)** √© um algoritmo para o problema do multi-armed bandit que equilibra explora√ß√£o e explora√ß√£o.
    
    **Como funciona:**
    1. **Explora√ß√£o vs Explora√ß√£o:** O algoritmo escolhe o bra√ßo que maximiza a recompensa estimada mais um b√¥nus de explora√ß√£o
    2. **F√≥rmula UCB1:** Escolhe o bra√ßo i que maximiza: Œº·µ¢ + ‚àö(2ln(t)/n·µ¢)
       - Œº·µ¢: recompensa m√©dia estimada do bra√ßo i
       - t: n√∫mero total de rodadas
       - n·µ¢: n√∫mero de vezes que o bra√ßo i foi escolhido
    3. **B√¥nus de Explora√ß√£o:** Diminui conforme o bra√ßo √© mais explorado, incentivando a explora√ß√£o de bra√ßos menos testados
    
    **M√©tricas Importantes:**
    - **Regret:** Diferen√ßa entre a recompensa √≥tima e a recompensa obtida
    - **Taxa de A√ß√µes √ìtimas:** Porcentagem de vezes que o melhor bra√ßo foi escolhido
    """)
