import streamlit as st
import matplotlib.pyplot as plt
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from scipy.stats import nbinom
import pandas as pd

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Simula√ß√£o UCB1 Multi-Armed Bandit",
    page_icon="üé∞",
    layout="wide"
)

# T√≠tulo principal
st.title("üé∞ Simula√ß√£o Interativa do Algoritmo UCB1 Multi-Armed Bandit")
st.markdown("### Cr√©ditos: github.com/petroud/E-greedy_and_UCB_algorithms/")

# Sidebar para controles
st.sidebar.header("Par√¢metros de Simula√ß√£o")

# Controles interativos
k = st.sidebar.slider("N√∫mero de bra√ßos (k)", min_value=2, max_value=20, value=10, help="N√∫mero de m√°quinas ca√ßa-n√≠queis")
T = st.sidebar.slider("N√∫mero de tentativas (T)", min_value=100, max_value=20000, value=10000, step=100, help="N√∫mero total de rodadas")

# Bot√£o para executar simula√ß√£o
run_simulation = st.sidebar.button("Executar Simula√ß√£o", type="primary")

# Seed para reprodutibilidade
seed = st.sidebar.number_input("Seed (opcional)", min_value=0, max_value=9999, value=42, help="Para resultados reproduz√≠veis")

def run_ucb1_simulation(k, T, seed=None):
    """Executa a simula√ß√£o do algoritmo UCB1"""
    if seed is not None:
        np.random.seed(seed)
    
    # Vetores do modelo
    ucb_pulls = np.zeros(k)
    ucb_estimate_M = np.zeros(k)
    ucb_total_rewards = np.zeros(k)
    ucb_inst_score = np.zeros(T)
    ucb_best_score = np.zeros(T)
    ucb_alg_score = np.zeros(T)
    ucb_regret = np.zeros(T)
    ucb_optimal_action = np.zeros(T)
    
    # Definindo distribui√ß√µes de probabilidade
    a = np.random.random(k)
    b = np.random.random(k)
    for i in range(k):
        if a[i] > b[i]:
            a[i], b[i] = b[i], a[i]
    
    mean = (a + b) / 2
    best = np.max(mean)
    index_best = np.where(mean == best)[0][0]
    
    def pull(i):
        """Puxa a alavanca i e retorna recompensa"""
        return np.random.uniform(a[i], b[i])
    
    success = 0
    
    def success_rate(optimal, i, total):
        """Calcula taxa de sucesso"""
        nonlocal success
        if i == index_best:
            success += 1
        optimal[total] = success / total if total > 0 else 0
    
    def update_stats_ucb(reward, i, t):
        """Atualiza estat√≠sticas do UCB"""
        ucb_pulls[i] += 1
        ucb_inst_score[t] = reward
        ucb_total_rewards[i] += reward
        ucb_best_score[t] = ucb_best_score[t-1] + best if t > 0 else best
        ucb_alg_score[t] = ucb_alg_score[t-1] + ucb_inst_score[t] if t > 0 else ucb_inst_score[t]
        ucb_estimate_M[i] = ucb_total_rewards[i] / ucb_pulls[i]
        ucb_regret[t] = (ucb_best_score[t] - ucb_alg_score[t]) / (t + 1)
    
    # Simula√ß√£o principal
    for t in range(T):
        if t == 0:
            # Primeira rodada: escolhe bra√ßo aleat√≥rio
            kth = np.random.randint(0, k)
        else:
            # UCB1: exploration bonus
            exploration_bonus = np.sqrt(np.log(t) / (ucb_pulls + 0.0001))
            kth = np.argmax(ucb_estimate_M + exploration_bonus)
        
        reward = pull(kth)
        update_stats_ucb(reward, kth, t)
        success_rate(ucb_optimal_action, kth, t)
    
    return {
        'regret': ucb_regret,
        'optimal_action': ucb_optimal_action,
        'mean_rewards': mean,
        'best_arm': index_best,
        'pulls': ucb_pulls,
        'total_rewards': ucb_total_rewards,
        'final_regret': ucb_regret[-1],
        'final_success_rate': ucb_optimal_action[-1],
        'a': a,
        'b': b
    }

# Execu√ß√£o da simula√ß√£o
if run_simulation or 'simulation_results' not in st.session_state:
    with st.spinner('Executando simula√ß√£o UCB1...'):
        results = run_ucb1_simulation(k, T, seed)
        st.session_state.simulation_results = results
        st.session_state.k = k
        st.session_state.T = T

# Verificar se h√° resultados para mostrar
if 'simulation_results' in st.session_state:
    results = st.session_state.simulation_results
    
    # Estat√≠sticas principais
    st.header("üìä Estat√≠sticas da Simula√ß√£o")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Melhor Bra√ßo", f"#{results['best_arm']}")
    
    with col2:
        st.metric("Taxa de Sucesso Final", f"{results['final_success_rate']:.2%}")
    
    with col3:
        st.metric("Regret Final", f"{results['final_regret']:.4f}")
    
    with col4:
        best_reward = results['mean_rewards'][results['best_arm']]
        st.metric("Recompensa M√©dia do Melhor Bra√ßo", f"{best_reward:.4f}")
    
    # Gr√°ficos principais
    st.header("üìà Visualiza√ß√µes")
    
    # Layout em duas colunas para os gr√°ficos
    col_left, col_right = st.columns(2)
    
    with col_left:
        st.subheader("Regret ao Longo do Tempo")
        
        # Gr√°fico de Regret usando Plotly
        fig_regret = go.Figure()
        fig_regret.add_trace(go.Scatter(
            x=np.arange(1, st.session_state.T + 1),
            y=results['regret'],
            mode='lines',
            name='UCB1',
            line=dict(color='red', width=2)
        ))
        
        fig_regret.update_layout(
            title=f"Regret para T={st.session_state.T} rodadas e k={st.session_state.k} bandits",
            xaxis_title="Rodada T",
            yaxis_title="Regret Total",
            hovermode='x'
        )
        
        st.plotly_chart(fig_regret, use_container_width=True)
    
    with col_right:
        st.subheader("Porcentagem de A√ß√µes √ìtimas")
        
        # Gr√°fico de A√ß√µes √ìtimas usando Plotly
        fig_optimal = go.Figure()
        fig_optimal.add_trace(go.Scatter(
            x=np.arange(1, st.session_state.T + 1),
            y=results['optimal_action'] * 100,  # Converter para porcentagem
            mode='lines',
            name='UCB1',
            line=dict(color='blue', width=2)
        ))
        
        fig_optimal.update_layout(
            title=f"A√ß√£o √≥tima tomada para T={st.session_state.T} rodadas e k={st.session_state.k} bandits",
            xaxis_title="Rodada T",
            yaxis_title="% A√ß√£o √≥tima tomada",
            hovermode='x'
        )
        
        st.plotly_chart(fig_optimal, use_container_width=True)
    
    # Gr√°fico da distribui√ß√£o de recompensas dos bra√ßos
    st.subheader("Distribui√ß√£o de Recompensas por Bra√ßo")
    
    # Criar DataFrame para melhor visualiza√ß√£o
    arms_data = pd.DataFrame({
        'Bra√ßo': [f'Bra√ßo {i}' for i in range(st.session_state.k)],
        'Recompensa M√©dia': results['mean_rewards'],
        'N√∫mero de Puxadas': results['pulls'],
        'Recompensa Total': results['total_rewards'],
        '√â o Melhor': ['Sim' if i == results['best_arm'] else 'N√£o' for i in range(st.session_state.k)]
    })
    
    # Gr√°fico de barras das recompensas m√©dias
    fig_arms = px.bar(
        arms_data, 
        x='Bra√ßo', 
        y='Recompensa M√©dia',
        color='√â o Melhor',
        color_discrete_map={'Sim': 'gold', 'N√£o': 'lightblue'},
        title="Recompensa M√©dia por Bra√ßo"
    )
    
    fig_arms.update_layout(
        xaxis_title="Bra√ßos",
        yaxis_title="Recompensa M√©dia",
        showlegend=True
    )
    
    st.plotly_chart(fig_arms, use_container_width=True)
    
    # Distribui√ß√£o Binomial Negativa
    st.subheader("Distribui√ß√£o Binomial Negativa")
    
    # Par√¢metros para a distribui√ß√£o binomial negativa
    st.sidebar.subheader("Par√¢metros Distribui√ß√£o Binomial Negativa")
    n_failures = st.sidebar.slider("N√∫mero de falhas (r)", min_value=1, max_value=50, value=10)
    prob_success = st.sidebar.slider("Probabilidade de sucesso (p)", min_value=0.01, max_value=0.99, value=0.3, step=0.01)
    
    # Gerar distribui√ß√£o binomial negativa
    x_values = np.arange(0, 100)
    nb_pmf = nbinom.pmf(x_values, n_failures, prob_success)
    
    fig_nb = go.Figure()
    fig_nb.add_trace(go.Scatter(
        x=x_values,
        y=nb_pmf,
        mode='lines+markers',
        name=f'Binomial Negativa (r={n_failures}, p={prob_success})',
        line=dict(color='green', width=2),
        marker=dict(size=4)
    ))
    
    fig_nb.update_layout(
        title=f"Distribui√ß√£o Binomial Negativa (r={n_failures}, p={prob_success})",
        xaxis_title="N√∫mero de sucessos",
        yaxis_title="Probabilidade",
        hovermode='x'
    )
    
    st.plotly_chart(fig_nb, use_container_width=True)
    
    # Tabela detalhada dos bra√ßos
    st.subheader("Detalhes dos Bra√ßos")
    st.dataframe(arms_data, use_container_width=True)
    
    # Informa√ß√µes adicionais sobre a distribui√ß√£o binomial negativa
    st.subheader("Informa√ß√µes sobre a Distribui√ß√£o Binomial Negativa")
    st.write(f"""
    A distribui√ß√£o binomial negativa modela o n√∫mero de sucessos em uma sequ√™ncia de tentativas independentes antes de obter um n√∫mero fixo de falhas.
    
    **Par√¢metros atuais:**
    - **r (n√∫mero de falhas):** {n_failures}
    - **p (probabilidade de sucesso):** {prob_success}
    
    **Estat√≠sticas:**
    - **M√©dia:** {n_failures * (1 - prob_success) / prob_success:.2f}
    - **Vari√¢ncia:** {n_failures * (1 - prob_success) / (prob_success ** 2):.2f}
    """)

else:
    st.info("üëÜ Configure os par√¢metros na barra lateral e clique em 'Executar Simula√ß√£o' para come√ßar!")
    
    # Explica√ß√£o do algoritmo
    st.header("‚ÑπÔ∏è Sobre o Algoritmo UCB1")
    st.write("""
    O **Upper Confidence Bound (UCB1)** √© um algoritmo para o problema do multi-armed bandit que equilibra explora√ß√£o e explora√ß√£o.
    
    **Como funciona:**
    1. **Explora√ß√£o vs Explora√ß√£o:** O algoritmo escolhe o bra√ßo que maximiza a recompensa estimada mais um b√¥nus de explora√ß√£o
    2. **F√≥rmula UCB1:** Escolhe o bra√ßo i que maximiza: Œº·µ¢ + ‚àö(2ln(t)/n·µ¢)
       - Œº·µ¢: recompensa m√©dia estimada do bra√ßo i
       - t: n√∫mero total de rodadas
       - n·µ¢: n√∫mero de vezes que o bra√ßo i foi escolhido
    3. **B√¥nus de Explora√ß√£o:** Diminui conforme o bra√ßo √© mais explorado, incentivando a explora√ß√£o de bra√ßos menos testados
    
    **M√©tricas Importantes:**
    - **Regret:** Diferen√ßa entre a recompensa √≥tima e a recompensa obtida
    - **Taxa de A√ß√µes √ìtimas:** Porcentagem de vezes que o melhor bra√ßo foi escolhido
    """)
