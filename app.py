
import streamlit as st
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import matplotlib.pyplot as plt
from scipy import stats
from scipy.stats import beta
import random
import math

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="üé∞ Simula√ß√£o Multi-Armed Bandit",
    page_icon="üé∞",
    layout="wide",
    initial_sidebar_state="expanded"
)

# T√≠tulo principal
st.title("üé∞ Simula√ß√£o Interativa de Algoritmos Multi-Armed Bandit")
st.markdown("Uma aplica√ß√£o web interativa para simular e comparar algoritmos de Multi-Armed Bandit")

# Classe para algoritmos Multi-Armed Bandit
class MultiArmedBandit:
    def __init__(self, k, seed=None):
        if seed is not None:
            np.random.seed(seed)
            random.seed(seed)
        
        self.k = k
        # Gera distribui√ß√µes uniformes aleat√≥rias para cada bra√ßo
        self.a = np.random.uniform(0, 1, k)
        self.b = self.a + np.random.uniform(0, 1, k)
        
        # Encontra o bra√ßo √≥timo
        self.optimal_arm = np.argmax((self.a + self.b) / 2)
        
    def pull(self, arm):
        """Retorna recompensa do bra√ßo especificado"""
        return np.random.uniform(self.a[arm], self.b[arm])
    
    def get_expected_rewards(self):
        """Retorna recompensas esperadas de todos os bra√ßos"""
        return (self.a + self.b) / 2

# Algoritmo UCB1
def ucb1_algorithm(bandit, T):
    k = bandit.k
    pulls = np.zeros(k)
    rewards = np.zeros(k)
    total_rewards = np.zeros(k)
    regrets = []
    optimal_actions = []
    
    for t in range(T):
        if t < k:
            # Primeira rodada: puxa cada bra√ßo uma vez
            chosen_arm = t
        else:
            # UCB1: exploration bonus
            exploration_bonus = np.sqrt(2 * np.log(t + 1) / (pulls + 1e-10))
            estimates = total_rewards / (pulls + 1e-10)
            ucb_values = estimates + exploration_bonus
            chosen_arm = np.argmax(ucb_values)
        
        # Puxa o bra√ßo escolhido
        reward = bandit.pull(chosen_arm)
        pulls[chosen_arm] += 1
        total_rewards[chosen_arm] += reward
        
        # Calcula regret
        optimal_reward = bandit.get_expected_rewards()[bandit.optimal_arm]
        current_regret = optimal_reward - reward
        regrets.append(current_regret)
        
        # Verifica se escolheu a√ß√£o √≥tima
        optimal_actions.append(chosen_arm == bandit.optimal_arm)
    
    return regrets, optimal_actions, pulls, total_rewards

# Algoritmo Epsilon-Greedy
def epsilon_greedy_algorithm(bandit, T, epsilon):
    k = bandit.k
    pulls = np.zeros(k)
    total_rewards = np.zeros(k)
    regrets = []
    optimal_actions = []
    
    for t in range(T):
        if np.random.random() < epsilon:
            # Explora√ß√£o: escolha aleat√≥ria
            chosen_arm = np.random.randint(k)
        else:
            # Explora√ß√£o: melhor bra√ßo atual
            estimates = total_rewards / (pulls + 1e-10)
            chosen_arm = np.argmax(estimates)
        
        # Puxa o bra√ßo escolhido
        reward = bandit.pull(chosen_arm)
        pulls[chosen_arm] += 1
        total_rewards[chosen_arm] += reward
        
        # Calcula regret
        optimal_reward = bandit.get_expected_rewards()[bandit.optimal_arm]
        current_regret = optimal_reward - reward
        regrets.append(current_regret)
        
        # Verifica se escolheu a√ß√£o √≥tima
        optimal_actions.append(chosen_arm == bandit.optimal_arm)
    
    return regrets, optimal_actions, pulls, total_rewards

# Algoritmo Thompson Sampling
def thompson_sampling_algorithm(bandit, T, alpha_prior=1, beta_prior=1):
    k = bandit.k
    pulls = np.zeros(k)
    total_rewards = np.zeros(k)
    regrets = []
    optimal_actions = []
    
    # Par√¢metros Beta para cada bra√ßo
    alpha = np.full(k, alpha_prior)
    beta_params = np.full(k, beta_prior)
    
    for t in range(T):
        # Amostra de distribui√ß√µes Beta
        samples = [np.random.beta(alpha[i], beta_params[i]) for i in range(k)]
        chosen_arm = np.argmax(samples)
        
        # Puxa o bra√ßo escolhido
        reward = bandit.pull(chosen_arm)
        pulls[chosen_arm] += 1
        total_rewards[chosen_arm] += reward
        
        # Normaliza recompensa para [0,1]
        normalized_reward = (reward - bandit.a[chosen_arm]) / (bandit.b[chosen_arm] - bandit.a[chosen_arm])
        
        # Atualiza distribui√ß√£o Beta
        alpha[chosen_arm] += normalized_reward
        beta_params[chosen_arm] += (1 - normalized_reward)
        
        # Calcula regret
        optimal_reward = bandit.get_expected_rewards()[bandit.optimal_arm]
        current_regret = optimal_reward - reward
        regrets.append(current_regret)
        
        # Verifica se escolheu a√ß√£o √≥tima
        optimal_actions.append(chosen_arm == bandit.optimal_arm)
    
    return regrets, optimal_actions, pulls, total_rewards

# Algoritmo Gradient Bandit
def gradient_bandit_algorithm(bandit, T, learning_rate=0.1):
    k = bandit.k
    pulls = np.zeros(k)
    total_rewards = np.zeros(k)
    regrets = []
    optimal_actions = []
    
    # Prefer√™ncias iniciais
    preferences = np.zeros(k)
    baseline = 0
    
    for t in range(T):
        # Calcula probabilidades softmax
        exp_prefs = np.exp(preferences - np.max(preferences))  # Estabilidade num√©rica
        probabilities = exp_prefs / np.sum(exp_prefs)
        
        # Escolhe bra√ßo baseado nas probabilidades
        chosen_arm = np.random.choice(k, p=probabilities)
        
        # Puxa o bra√ßo escolhido
        reward = bandit.pull(chosen_arm)
        pulls[chosen_arm] += 1
        total_rewards[chosen_arm] += reward
        
        # Atualiza baseline
        baseline = (baseline * t + reward) / (t + 1)
        
        # Atualiza prefer√™ncias
        for arm in range(k):
            if arm == chosen_arm:
                preferences[arm] += learning_rate * (reward - baseline) * (1 - probabilities[arm])
            else:
                preferences[arm] -= learning_rate * (reward - baseline) * probabilities[arm]
        
        # Calcula regret
        optimal_reward = bandit.get_expected_rewards()[bandit.optimal_arm]
        current_regret = optimal_reward - reward
        regrets.append(current_regret)
        
        # Verifica se escolheu a√ß√£o √≥tima
        optimal_actions.append(chosen_arm == bandit.optimal_arm)
    
    return regrets, optimal_actions, pulls, total_rewards

# Fun√ß√£o para plotar resultados
def plot_results(regrets_dict, optimal_actions_dict, T):
    # Cria subplots verticais (um em cima do outro)
    fig = make_subplots(
        rows=2, cols=1,
        subplot_titles=('Regret M√©dio Acumulado', 'Taxa de A√ß√µes √ìtimas'),
        vertical_spacing=0.12,
        specs=[[{"secondary_y": False}], [{"secondary_y": False}]]
    )
    
    colors = {'UCB1': '#1f77b4', 'Epsilon-Greedy': '#ff7f0e', 
              'Thompson Sampling': '#2ca02c', 'Gradient Bandit': '#d62728'}
    
    for algo_name in regrets_dict.keys():
        # Regret cumulativo m√©dio
        cumulative_regret = np.cumsum(regrets_dict[algo_name]) / np.arange(1, T + 1)
        
        # Taxa de a√ß√µes √≥timas
        optimal_rate = np.cumsum(optimal_actions_dict[algo_name]) / np.arange(1, T + 1) * 100
        
        # Adiciona traces
        fig.add_trace(
            go.Scatter(x=list(range(1, T + 1)), y=cumulative_regret,
                      mode='lines', name=f'{algo_name}',
                      line=dict(color=colors.get(algo_name, '#000000'))),
            row=1, col=1
        )
        
        fig.add_trace(
            go.Scatter(x=list(range(1, T + 1)), y=optimal_rate,
                      mode='lines', name=f'{algo_name}',
                      line=dict(color=colors.get(algo_name, '#000000')),
                      showlegend=False),
            row=2, col=1
        )
    
    # Atualiza layout
    fig.update_xaxes(title_text="Tentativas", row=1, col=1)
    fig.update_xaxes(title_text="Tentativas", row=2, col=1)
    fig.update_yaxes(title_text="Regret M√©dio", row=1, col=1)
    fig.update_yaxes(title_text="Taxa de A√ß√µes √ìtimas (%)", row=2, col=1)
    
    fig.update_layout(height=700, showlegend=True)
    
    return fig

# Fun√ß√£o para plotar distribui√ß√µes dos bra√ßos
def plot_arm_distributions(bandit, pulls, total_rewards):
    fig = go.Figure()
    
    expected_rewards = bandit.get_expected_rewards()
    actual_rewards = total_rewards / (pulls + 1e-10)
    
    arms = list(range(bandit.k))
    
    # Barra para recompensas esperadas
    fig.add_trace(go.Bar(
        x=arms,
        y=expected_rewards,
        name='Recompensa Esperada',
        marker_color='lightblue',
        opacity=0.7
    ))
    
    # Barra para recompensas observadas
    fig.add_trace(go.Bar(
        x=arms,
        y=actual_rewards,
        name='Recompensa Observada',
        marker_color='darkblue',
        opacity=0.8
    ))
    
    # Destaca bra√ßo √≥timo
    optimal_arm = bandit.optimal_arm
    fig.add_shape(
        type="rect",
        x0=optimal_arm-0.4, y0=0,
        x1=optimal_arm+0.4, y1=max(expected_rewards)*1.1,
        line=dict(color="red", width=3),
        fillcolor="rgba(255,0,0,0.1)"
    )
    
    fig.update_layout(
        title="Distribui√ß√£o de Recompensas por Bra√ßo",
        xaxis_title="Bra√ßo",
        yaxis_title="Recompensa",
        barmode='group',
        height=400
    )
    
    return fig

# Sidebar com controles
st.sidebar.title("‚öôÔ∏è Configura√ß√µes")

# Sele√ß√£o do modo
mode = st.sidebar.selectbox(
    "ü§ñ Modo de Simula√ß√£o",
    ["Algoritmo Individual", "Compara√ß√£o Completa"],
    help="Execute um algoritmo ou compare todos simultaneamente"
)

# Par√¢metros b√°sicos
st.sidebar.subheader("üìä Par√¢metros B√°sicos")

k = st.sidebar.slider("N√∫mero de bra√ßos (k)", 2, 50, 10, help="N√∫mero de op√ß√µes dispon√≠veis")
T = st.sidebar.slider("N√∫mero de tentativas (T)", 100, 50000, 1000, help="N√∫mero total de rounds da simula√ß√£o")
seed = st.sidebar.number_input("Seed (opcional)", value=42, help="Para resultados reproduz√≠veis")

if mode == "Algoritmo Individual":
    algorithm = st.sidebar.selectbox(
        "Algoritmo",
        ["UCB1", "Epsilon-Greedy", "Thompson Sampling", "Gradient Bandit"]
    )
    
    # Par√¢metros espec√≠ficos do algoritmo
    if algorithm == "Epsilon-Greedy":
        st.sidebar.markdown("**üìö Explica√ß√£o do Epsilon:**")
        st.sidebar.markdown("""
        - **Œµ = 0.01**: 1% explora√ß√£o, 99% explora√ß√£o (conservador)
        - **Œµ = 0.1**: 10% explora√ß√£o, 90% explora√ß√£o (balanceado)
        - **Œµ = 0.3**: 30% explora√ß√£o, 70% explora√ß√£o (explorat√≥rio)
        """)
        epsilon = st.sidebar.slider("Epsilon (Œµ)", 0.01, 0.5, 0.1, 0.01, 
                                   help="Probabilidade de escolher a√ß√£o aleat√≥ria para explorar")
        
    elif algorithm == "Thompson Sampling":
        st.sidebar.markdown("**üìö Explica√ß√£o dos Priors:**")
        st.sidebar.markdown("""
        - **Œ± = Œ≤ = 1**: Prior neutro (distribui√ß√£o uniforme)
        - **Œ± > Œ≤**: Expectativa otimista (espera mais sucessos)
        - **Œ± < Œ≤**: Expectativa pessimista (espera mais falhas)
        - **Valores altos**: Maior confian√ßa no prior
        """)
        alpha_prior = st.sidebar.slider("Alpha Prior (Œ±)", 0.1, 10.0, 1.0, 0.1, 
                                       help="Prior para sucessos - valores altos = mais otimista")
        beta_prior = st.sidebar.slider("Beta Prior (Œ≤)", 0.1, 10.0, 1.0, 0.1, 
                                      help="Prior para falhas - valores altos = mais conservador")
        
    elif algorithm == "Gradient Bandit":
        st.sidebar.markdown("**üìö Explica√ß√£o do Learning Rate:**")
        st.sidebar.markdown("""
        - **Œ± = 0.01**: Aprendizado muito lento mas est√°vel
        - **Œ± = 0.1**: Aprendizado moderado (recomendado)
        - **Œ± = 0.5**: Aprendizado r√°pido mas pode oscilar
        - **Œ± = 1.0**: Muito r√°pido, pode ser inst√°vel
        """)
        learning_rate = st.sidebar.slider("Learning Rate (Œ±)", 0.01, 1.0, 0.1, 0.01, 
                                         help="Velocidade de atualiza√ß√£o das prefer√™ncias")

# Controles adicionais
st.sidebar.subheader("üéõÔ∏è Controles")
show_arm_details = st.sidebar.checkbox("‚úÖ Mostrar detalhes dos bra√ßos", value=True)
show_distribution = st.sidebar.checkbox("‚úÖ Mostrar distribui√ß√£o binomial negativa", value=False)

# Informa√ß√µes sobre algoritmos
with st.sidebar.expander("‚ÑπÔ∏è Informa√ß√µes sobre Algoritmos"):
    st.markdown("""
    **UCB1 (Upper Confidence Bound)**
    - F√≥rmula: Œº·µ¢ + ‚àö(2ln(t)/n·µ¢)
    - Equilibra explora√ß√£o/explora√ß√£o usando intervalos de confian√ßa
    - Garantias te√≥ricas de regret logar√≠tmico
    - Sem par√¢metros para ajustar!
    
    **Epsilon-Greedy**
    - Explora aleatoriamente com probabilidade Œµ
    - Œµ baixo (0.01): mais explora√ß√£o, converg√™ncia lenta
    - Œµ alto (0.3): mais explora√ß√£o, pode n√£o convergir
    - Œµ ideal: geralmente entre 0.05-0.15
    
    **Thompson Sampling**
    - Abordagem Bayesiana com distribui√ß√µes Beta
    - Œ± (alpha): prior de sucessos, valores altos = mais otimista
    - Œ≤ (beta): prior de falhas, valores altos = mais conservador
    - Valores iguais (Œ±=Œ≤=1): prior neutro
    
    **Gradient Bandit**
    - Aprende prefer√™ncias usando gradiente estoc√°stico
    - Learning Rate baixo (0.01): aprendizado lento mas est√°vel
    - Learning Rate alto (0.5): aprendizado r√°pido mas inst√°vel
    - Usa baseline para reduzir vari√¢ncia
    """)

# Bot√£o para executar simula√ß√£o
if st.sidebar.button("üöÄ Executar Simula√ß√£o", type="primary"):
    with st.spinner("Executando simula√ß√£o..."):
        # Cria bandit
        bandit = MultiArmedBandit(k, seed)
        
        if mode == "Algoritmo Individual":
            # Executa algoritmo individual
            if algorithm == "UCB1":
                regrets, optimal_actions, pulls, total_rewards = ucb1_algorithm(bandit, T)
            elif algorithm == "Epsilon-Greedy":
                regrets, optimal_actions, pulls, total_rewards = epsilon_greedy_algorithm(bandit, T, epsilon)
            elif algorithm == "Thompson Sampling":
                regrets, optimal_actions, pulls, total_rewards = thompson_sampling_algorithm(bandit, T, alpha_prior, beta_prior)
            elif algorithm == "Gradient Bandit":
                regrets, optimal_actions, pulls, total_rewards = gradient_bandit_algorithm(bandit, T, learning_rate)
            
            # M√©tricas
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Regret Final", f"{np.mean(regrets):.4f}")
            with col2:
                st.metric("Taxa de Sucesso", f"{np.mean(optimal_actions)*100:.1f}%")
            with col3:
                st.metric("Bra√ßo √ìtimo", f"{bandit.optimal_arm}")
            
            # Gr√°ficos
            regrets_dict = {algorithm: regrets}
            optimal_actions_dict = {algorithm: optimal_actions}
            
            fig = plot_results(regrets_dict, optimal_actions_dict, T)
            st.plotly_chart(fig, use_container_width=True)
            
            # Distribui√ß√£o dos bra√ßos
            if show_arm_details:
                st.subheader("üìä An√°lise Detalhada dos Bra√ßos")
                
                fig_dist = plot_arm_distributions(bandit, pulls, total_rewards)
                st.plotly_chart(fig_dist, use_container_width=True)
                
                # Tabela de estat√≠sticas
                df_stats = pd.DataFrame({
                    'Bra√ßo': range(k),
                    'Recompensa Esperada': bandit.get_expected_rewards(),
                    'Recompensa Observada': total_rewards / (pulls + 1e-10),
                    'N√∫mero de Puxadas': pulls.astype(int),
                    'Recompensa Total': total_rewards,
                    '√â √ìtimo': ['‚úÖ' if i == bandit.optimal_arm else '‚ùå' for i in range(k)]
                })
                
                st.dataframe(df_stats, use_container_width=True)
        
        else:  # Compara√ß√£o Completa
            # Executa todos os algoritmos
            algorithms = {
                'UCB1': lambda: ucb1_algorithm(bandit, T),
                'Epsilon-Greedy': lambda: epsilon_greedy_algorithm(bandit, T, 0.1),
                'Thompson Sampling': lambda: thompson_sampling_algorithm(bandit, T, 1.0, 1.0),
                'Gradient Bandit': lambda: gradient_bandit_algorithm(bandit, T, 0.1)
            }
            
            regrets_dict = {}
            optimal_actions_dict = {}
            results_summary = {}
            
            for algo_name, algo_func in algorithms.items():
                regrets, optimal_actions, pulls, total_rewards = algo_func()
                regrets_dict[algo_name] = regrets
                optimal_actions_dict[algo_name] = optimal_actions
                results_summary[algo_name] = {
                    'regret_final': np.mean(regrets),
                    'taxa_sucesso': np.mean(optimal_actions) * 100
                }
            
            # M√©tricas comparativas
            st.subheader("üìà Compara√ß√£o de Performance")
            
            cols = st.columns(4)
            for i, (algo_name, metrics) in enumerate(results_summary.items()):
                with cols[i]:
                    st.metric(
                        f"{algo_name}",
                        f"Regret: {metrics['regret_final']:.4f}",
                        f"Sucesso: {metrics['taxa_sucesso']:.1f}%"
                    )
            
            # Gr√°fico comparativo
            fig = plot_results(regrets_dict, optimal_actions_dict, T)
            st.plotly_chart(fig, use_container_width=True)
            
            # Tabela de compara√ß√£o
            df_comparison = pd.DataFrame(results_summary).T
            df_comparison.columns = ['Regret Final', 'Taxa de Sucesso (%)']
            df_comparison['Ranking Regret'] = df_comparison['Regret Final'].rank()
            df_comparison['Ranking Sucesso'] = df_comparison['Taxa de Sucesso (%)'].rank(ascending=False)
            
            st.dataframe(df_comparison.style.highlight_min(subset=['Regret Final'], color='lightgreen')
                                           .highlight_max(subset=['Taxa de Sucesso (%)'], color='lightgreen'),
                        use_container_width=True)

# Distribui√ß√£o binomial negativa (educacional)
if show_distribution:
    st.subheader("üìä Distribui√ß√£o Binomial Negativa (Educacional)")
    
    # Explica√ß√£o te√≥rica
    with st.expander("üéì O que √© a Distribui√ß√£o Binomial Negativa?"):
        st.markdown("""
        A **Distribui√ß√£o Binomial Negativa** modela o n√∫mero de sucessos que ocorrem antes de um n√∫mero fixo de falhas.
        
        **Aplica√ß√µes em Multi-Armed Bandit:**
        - Modelar tempo at√© encontrar o bra√ßo √≥timo
        - Representar distribui√ß√µes de recompensa com maior variabilidade
        - Simular cen√°rios mais realistas que a distribui√ß√£o uniforme
        
        **F√≥rmula:** P(X = k) = C(k+r-1, k) √ó p^k √ó (1-p)^r
        
        **Interpreta√ß√£o:**
        - **r**: N√∫mero de falhas desejadas
        - **p**: Probabilidade de sucesso em cada tentativa
        - **X**: N√∫mero de sucessos antes de r falhas
        """)
    
    col1, col2 = st.columns([1, 2])
    with col1:
        st.markdown("**üìö Configura√ß√£o dos Par√¢metros:**")
        
        r = st.slider("Par√¢metro r (falhas)", 1, 20, 5, 
                     help="N√∫mero de falhas antes de parar o experimento")
        p = st.slider("Par√¢metro p (probabilidade)", 0.01, 0.99, 0.3, 0.01,
                     help="Probabilidade de sucesso em cada tentativa")
        
        # Explica√ß√£o dos par√¢metros atuais
        st.markdown(f"""
        **Interpreta√ß√£o Atual:**
        - Esperamos **{r} falhas** antes de parar
        - Cada tentativa tem **{p:.1%}** chance de sucesso
        - M√©dia esperada: **{r*p/(1-p):.2f}** sucessos
        - Vari√¢ncia: **{r*p/((1-p)**2):.2f}**
        """)
        
        # Casos especiais
        if p < 0.1:
            st.warning("‚ö†Ô∏è Probabilidade muito baixa - poucos sucessos esperados")
        elif p > 0.8:
            st.info("‚ÑπÔ∏è Probabilidade alta - muitos sucessos esperados")
        
        if r == 1:
            st.info("‚ÑπÔ∏è r=1: Distribui√ß√£o Geom√©trica (caso especial)")
    
    with col2:
        x = np.arange(0, min(50, int(r*p/(1-p)*3 + 20)))  # Range adaptativo
        pmf = stats.nbinom.pmf(x, r, p)
        
        fig = go.Figure()
        fig.add_trace(go.Bar(
            x=x, 
            y=pmf, 
            name='PMF',
            marker_color='steelblue',
            hovertemplate='<b>Sucessos:</b> %{x}<br><b>Probabilidade:</b> %{y:.4f}<extra></extra>'
        ))
        
        # Adiciona linha da m√©dia
        mean_val = r*p/(1-p)
        fig.add_vline(x=mean_val, line_dash="dash", line_color="red", 
                     annotation_text=f"M√©dia: {mean_val:.2f}")
        
        fig.update_layout(
            title=f"Distribui√ß√£o Binomial Negativa (r={r}, p={p:.2f})",
            xaxis_title="N√∫mero de Sucessos",
            yaxis_title="Probabilidade",
            height=400,
            showlegend=False
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # Estat√≠sticas resumidas
        st.markdown(f"""
        **üìä Estat√≠sticas da Distribui√ß√£o:**
        - **M√©dia (Œº):** {r*p/(1-p):.3f}
        - **Vari√¢ncia (œÉ¬≤):** {r*p/((1-p)**2):.3f}
        - **Desvio Padr√£o (œÉ):** {np.sqrt(r*p/((1-p)**2)):.3f}
        - **Moda:** {max(0, int((r-1)*p/(1-p))) if r > 1 else 0}
        """)

# Se√ß√£o educacional sobre conceitos fundamentais
with st.expander("üéì Conceitos Fundamentais do Multi-Armed Bandit"):
    st.markdown("""
    ### ü§î O Dilema Explora√ß√£o vs. Explora√ß√£o
    
    **Explora√ß√£o (Exploration):** Tentar op√ß√µes desconhecidas para descobrir se podem ser melhores.
    **Explora√ß√£o (Exploitation):** Usar o conhecimento atual para maximizar a recompensa.
    
    ### üìà M√©tricas Importantes
    
    **Regret:** Diferen√ßa entre a recompensa √≥tima e a obtida
    - Regret baixo = algoritmo eficiente
    - Regret cresce com o tempo em algoritmos ruins
    
    **Taxa de A√ß√µes √ìtimas:** Porcentagem de vezes que escolheu a melhor op√ß√£o
    - 100% = sempre escolheu o melhor bra√ßo (improv√°vel no in√≠cio)
    - Cresce com o tempo em algoritmos bons
    
    ### üîÑ Como os Algoritmos Funcionam
    
    **UCB1:** Calcula um "limite superior de confian√ßa" para cada bra√ßo. Escolhe o bra√ßo com maior valor UCB.
    
    **Epsilon-Greedy:** Na maioria das vezes (1-Œµ) escolhe o melhor bra√ßo conhecido. Ocasionalmente (Œµ) explora aleatoriamente.
    
    **Thompson Sampling:** Mant√©m uma distribui√ß√£o de probabilidade para cada bra√ßo e amostra para decidir.
    
    **Gradient Bandit:** Aprende "prefer√™ncias" por cada bra√ßo e usa probabilidades para escolher.
    
    ### üí° Aplica√ß√µes Pr√°ticas
    - **A/B Testing:** Qual vers√£o de website converte mais?
    - **Sistemas de Recomenda√ß√£o:** Qual produto recomendar?
    - **Publicidade Online:** Qual an√∫ncio gera mais cliques?
    - **Otimiza√ß√£o de Tratamentos M√©dicos:** Qual medicamento √© mais eficaz?
    """)

# Footer
st.markdown("---")
st.markdown("*Desenvolvido com ‚ù§Ô∏è para demonstrar os conceitos fundamentais de Multi-Armed Bandit de forma interativa e educacional.*")
